{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banchmark fully off-policy DQN on Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is stable-baselines **Behavior Cloning** the right choice to make an off-policy dataset?\n",
    "* The dokumentation says: _\"for a given observation, the action taken by the policy must be the one taken by the expert\"_\n",
    "* That´s in my opption not the same as filling a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting off-policy data to tranin DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is saved as expert_cartpole.npz in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (878987, 1)\n",
      "obs (878987, 4)\n",
      "rewards (878987,)\n",
      "episode_returns (10000,)\n",
      "episode_starts (878987,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'actions': array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [1],\n",
       "        [1]]),\n",
       " 'obs': array([[-0.03360261,  0.04453236, -0.03984775,  0.01214404],\n",
       "        [-0.03271196, -0.14999613, -0.03960487,  0.29199302],\n",
       "        [-0.03571189, -0.34453166, -0.03376501,  0.5719267 ],\n",
       "        ...,\n",
       "        [ 1.8527644 ,  2.7364528 ,  0.20202377, -0.023985  ],\n",
       "        [ 1.9074935 ,  2.5390933 ,  0.20154408,  0.32502544],\n",
       "        [ 1.9582753 ,  2.730861  ,  0.20804459,  0.10205026]],\n",
       "       dtype=float32),\n",
       " 'rewards': array([1., 1., 1., ..., 1., 1., 1.]),\n",
       " 'episode_returns': array([91., 85., 85., ..., 88., 82., 92.]),\n",
       " 'episode_starts': array([ True, False, False, ..., False, False, False])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines.gail import generate_expert_traj\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=0)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e6), n_episodes=int(1e4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning DQN with fully off-policy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I´m note sure if `model.pretrain()` is the same like a `replay_buffer`\n",
    "* The Dokumentation says: _\"supervised learning given an expert dataset\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (878987, 1)\n",
      "obs (878987, 4)\n",
      "rewards (878987,)\n",
      "episode_returns (10000,)\n",
      "episode_starts (878987,)\n",
      "Total trajectories: -1\n",
      "Total transitions: 878987\n",
      "Average returns: 87.8987\n",
      "Std for returns: 5.305189752497077\n",
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n",
      "Pretraining with Behavior Cloning...\n",
      "==== Training progress 10.00% ====\n",
      "Epoch 10\n",
      "Training loss: 0.073304, Validation loss: 0.071991\n",
      "\n",
      "==== Training progress 20.00% ====\n",
      "Epoch 20\n",
      "Training loss: 0.057887, Validation loss: 0.056496\n",
      "\n",
      "==== Training progress 30.00% ====\n",
      "Epoch 30\n",
      "Training loss: 0.051736, Validation loss: 0.049762\n",
      "\n",
      "==== Training progress 40.00% ====\n",
      "Epoch 40\n",
      "Training loss: 0.048213, Validation loss: 0.047909\n",
      "\n",
      "==== Training progress 50.00% ====\n",
      "Epoch 50\n",
      "Training loss: 0.046434, Validation loss: 0.044619\n",
      "\n",
      "==== Training progress 60.00% ====\n",
      "Epoch 60\n",
      "Training loss: 0.045257, Validation loss: 0.042969\n",
      "\n",
      "==== Training progress 70.00% ====\n",
      "Epoch 70\n",
      "Training loss: 0.044263, Validation loss: 0.042591\n",
      "\n",
      "==== Training progress 80.00% ====\n",
      "Epoch 80\n",
      "Training loss: 0.043583, Validation loss: 0.041198\n",
      "\n",
      "==== Training progress 90.00% ====\n",
      "Epoch 90\n",
      "Training loss: 0.042937, Validation loss: 0.041382\n",
      "\n",
      "==== Training progress 100.00% ====\n",
      "Epoch 100\n",
      "Training loss: 0.042629, Validation loss: 0.040169\n",
      "\n",
      "Pretraining done.\n",
      "92.0\n",
      "85.0\n",
      "92.0\n",
      "87.0\n",
      "89.0\n",
      "80.0\n",
      "87.0\n",
      "87.0\n",
      "78.0\n",
      "89.0\n",
      "89.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='expert_cartpole.npz',\n",
    "                        traj_limitation=-1, batch_size=128)\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "# Pretrain the DQN\n",
    "model.pretrain(dataset, n_epochs=100)\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "#         env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_buffer(self, total_timesteps, callback=None, log_interval=100, tb_log_name=\"DQN\",\n",
    "              reset_num_timesteps=True, replay_wrapper=None):\n",
    "\n",
    "        new_tb_log = self._init_num_timesteps(reset_num_timesteps)\n",
    "        callback = self._init_callback(callback)\n",
    "\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name, new_tb_log) \\\n",
    "                as writer:\n",
    "            self._setup_learn()\n",
    "\n",
    "            # Create the replay buffer\n",
    "#             if self.prioritized_replay:\n",
    "#                 self.replay_buffer = PrioritizedReplayBuffer(self.buffer_size, alpha=self.prioritized_replay_alpha)\n",
    "#                 if self.prioritized_replay_beta_iters is None:\n",
    "#                     prioritized_replay_beta_iters = total_timesteps\n",
    "#                 else:\n",
    "#                     prioritized_replay_beta_iters = self.prioritized_replay_beta_iters\n",
    "#                 self.beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
    "#                                                     initial_p=self.prioritized_replay_beta0,\n",
    "#                                                     final_p=1.0)\n",
    "#             else:\n",
    "            self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "            self.beta_schedule = None\n",
    "\n",
    "#             if replay_wrapper is not None:\n",
    "#                 assert not self.prioritized_replay, \"Prioritized replay buffer is not supported by HER\"\n",
    "#                 self.replay_buffer = replay_wrapper(self.replay_buffer)\n",
    "\n",
    "            # Create the schedule for exploration starting from 1.\n",
    "#             self.exploration = LinearSchedule(schedule_timesteps=int(self.exploration_fraction * total_timesteps),\n",
    "#                                               initial_p=self.exploration_initial_eps,\n",
    "#                                               final_p=self.exploration_final_eps)\n",
    "\n",
    "            episode_rewards = [0.0]\n",
    "            episode_successes = []\n",
    "\n",
    "            callback.on_training_start(locals(), globals())\n",
    "#             callback.on_rollout_start()\n",
    "\n",
    "            reset = True\n",
    "            obs = self.env.reset()\n",
    "            # Retrieve unnormalized observation for saving into the buffer\n",
    "            if self._vec_normalize_env is not None:\n",
    "                obs_ = self._vec_normalize_env.get_original_obs().squeeze()\n",
    "\n",
    "            for _ in range(total_timesteps):\n",
    "                # Take action and update exploration to the newest value\n",
    "                kwargs = {}\n",
    "                if not self.param_noise:\n",
    "                    update_eps = self.exploration.value(self.num_timesteps)\n",
    "                    update_param_noise_threshold = 0.\n",
    "                else:\n",
    "                    update_eps = 0.\n",
    "                    # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
    "                    # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
    "                    # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
    "                    # for detailed explanation.\n",
    "                    update_param_noise_threshold = \\\n",
    "                        -np.log(1. - self.exploration.value(self.num_timesteps) +\n",
    "                                self.exploration.value(self.num_timesteps) / float(self.env.action_space.n))\n",
    "                    kwargs['reset'] = reset\n",
    "                    kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
    "                    kwargs['update_param_noise_scale'] = True\n",
    "#                 with self.sess.as_default():\n",
    "#                     action = self.act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
    "#                 env_action = action\n",
    "#                 reset = False\n",
    "#                 new_obs, rew, done, info = self.env.step(env_action)\n",
    "\n",
    "                self.num_timesteps += 1\n",
    "\n",
    "#                 # Stop training if return value is False\n",
    "#                 if callback.on_step() is False:\n",
    "#                     break\n",
    "\n",
    "#                 # Store only the unnormalized version\n",
    "#                 if self._vec_normalize_env is not None:\n",
    "#                     new_obs_ = self._vec_normalize_env.get_original_obs().squeeze()\n",
    "#                     reward_ = self._vec_normalize_env.get_original_reward().squeeze()\n",
    "#                 else:\n",
    "#                     # Avoid changing the original ones\n",
    "#                     obs_, new_obs_, reward_ = obs, new_obs, rew\n",
    "#                 # Store transition in the replay buffer.\n",
    "#                 self.replay_buffer.add(obs_, action, reward_, new_obs_, float(done))\n",
    "#                 obs = new_obs\n",
    "#                 # Save the unnormalized observation\n",
    "#                 if self._vec_normalize_env is not None:\n",
    "#                     obs_ = new_obs_\n",
    "\n",
    "                if writer is not None:\n",
    "                    ep_rew = np.array([reward_]).reshape((1, -1))\n",
    "                    ep_done = np.array([done]).reshape((1, -1))\n",
    "                    tf_util.total_episode_reward_logger(self.episode_reward, ep_rew, ep_done, writer,\n",
    "                                                        self.num_timesteps)\n",
    "\n",
    "#                 episode_rewards[-1] += reward_\n",
    "#                 if done:\n",
    "#                     maybe_is_success = info.get('is_success')\n",
    "#                     if maybe_is_success is not None:\n",
    "#                         episode_successes.append(float(maybe_is_success))\n",
    "#                     if not isinstance(self.env, VecEnv):\n",
    "#                         obs = self.env.reset()\n",
    "#                     episode_rewards.append(0.0)\n",
    "                    reset = True\n",
    "\n",
    "                # Do not train if the warmup phase is not over\n",
    "                # or if there are not enough samples in the replay buffer\n",
    "                can_sample = self.replay_buffer.can_sample(self.batch_size)\n",
    "#                 if can_sample and self.num_timesteps > self.learning_starts \\\n",
    "#                         and self.num_timesteps % self.train_freq == 0:\n",
    "\n",
    "#                 callback.on_rollout_end()\n",
    "#                 # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "#                 # pytype:disable=bad-unpacking\n",
    "                if self.prioritized_replay:\n",
    "                    assert self.beta_schedule is not None, \\\n",
    "                           \"BUG: should be LinearSchedule when self.prioritized_replay True\"\n",
    "                    experience = self.replay_buffer.sample(self.batch_size,\n",
    "                                                           beta=self.beta_schedule.value(self.num_timesteps),\n",
    "                                                           env=self._vec_normalize_env)\n",
    "                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
    "                else:\n",
    "                    obses_t, actions, rewards, obses_tp1, dones = self.replay_buffer.sample(self.batch_size,\n",
    "                                                                                            env=self._vec_normalize_env)\n",
    "                    weights, batch_idxes = np.ones_like(rewards), None\n",
    "                # pytype:enable=bad-unpacking\n",
    "\n",
    "                if writer is not None:\n",
    "                    # run loss backprop with summary, but once every 100 steps save the metadata\n",
    "                    # (memory, compute time, ...)\n",
    "                    if (1 + self.num_timesteps) % 100 == 0:\n",
    "                        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                        run_metadata = tf.RunMetadata()\n",
    "                        summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                              dones, weights, sess=self.sess, options=run_options,\n",
    "                                                              run_metadata=run_metadata)\n",
    "                        writer.add_run_metadata(run_metadata, 'step%d' % self.num_timesteps)\n",
    "                    else:\n",
    "                        summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                              dones, weights, sess=self.sess)\n",
    "                    writer.add_summary(summary, self.num_timesteps)\n",
    "                else:\n",
    "                    _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights,\n",
    "                                                    sess=self.sess)\n",
    "\n",
    "                if self.prioritized_replay:\n",
    "                    new_priorities = np.abs(td_errors) + self.prioritized_replay_eps\n",
    "                    assert isinstance(self.replay_buffer, PrioritizedReplayBuffer)\n",
    "                    self.replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "\n",
    "#                 callback.on_rollout_start()\n",
    "\n",
    "#                 if can_sample and self.num_timesteps > self.learning_starts and \\\n",
    "#                         self.num_timesteps % self.target_network_update_freq == 0:\n",
    "#                     # Update target network periodically.\n",
    "#                     self.update_target(sess=self.sess)\n",
    "\n",
    "#                 if len(episode_rewards[-101:-1]) == 0:\n",
    "#                     mean_100ep_reward = -np.inf\n",
    "#                 else:\n",
    "#                     mean_100ep_reward = round(float(np.mean(episode_rewards[-101:-1])), 1)\n",
    "\n",
    "#                 num_episodes = len(episode_rewards)\n",
    "#                 if self.verbose >= 1 and done and log_interval is not None and len(episode_rewards) % log_interval == 0:\n",
    "#                     logger.record_tabular(\"steps\", self.num_timesteps)\n",
    "#                     logger.record_tabular(\"episodes\", num_episodes)\n",
    "#                     if len(episode_successes) > 0:\n",
    "#                         logger.logkv(\"success rate\", np.mean(episode_successes[-100:]))\n",
    "#                     logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
    "#                     logger.record_tabular(\"% time spent exploring\",\n",
    "#                                           int(100 * self.exploration.value(self.num_timesteps)))\n",
    "#                     logger.dump_tabular()\n",
    "\n",
    "        callback.on_training_end()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mujoco",
   "language": "python",
   "name": "venv_mujoco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
