{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banchmark fully off-policy DQN on Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to Benachmark with three diffrent types of datasets generated by\n",
    "* a __partially-trained__ medium-return policy\n",
    "* a __random__ low-return policy and \n",
    "* an __expert__, high-return policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataste\n",
    "Load `.npz` file and save data in variabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03360261,  0.04453236, -0.03984775,  0.01214404],\n",
       "       [-0.03271196, -0.14999613, -0.03960487,  0.29199302],\n",
       "       [-0.03571189, -0.34453166, -0.03376501,  0.5719267 ],\n",
       "       ...,\n",
       "       [ 1.8527644 ,  2.7364528 ,  0.20202377, -0.023985  ],\n",
       "       [ 1.9074935 ,  2.5390933 ,  0.20154408,  0.32502544],\n",
       "       [ 1.9582753 ,  2.730861  ,  0.20804459,  0.10205026]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('cartpole_data.npz')\n",
    "obs = data['obs'] # shape = (878987, 4)\n",
    "actions = data['actions'] # shape = (878987, 1)\n",
    "rewards = data['rewards'] # shape = (878987, )\n",
    "episode_starts = data['episode_starts']\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning DQN with fully off-policy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable-Baselines\n",
    "[Stable Baselines](https://stable-baselines.readthedocs.io/en/master/index.html) is a set of improved implementations of reinforcement learning algorithms based on OpenAI [Baselines](https://github.com/openai/baselines/).\n",
    "### Pre-Training (Behavior Cloning)\n",
    "With the `.pretrain()` method, you can pre-train RL policies using trajectories from an expert, and therefore accelerate training. [Learn more...](https://stable-baselines.readthedocs.io/en/master/guide/pretrain.html)\n",
    "* Stable-Baselines `.pretrain()` pretrains a model using behavior cloning: supervised learning given an expert dataset.\n",
    "* The dokumentation says: _\"for a given observation, the action taken by the policy must be the one taken by the expert\"_\n",
    "* Since Q-Learning has no Policy this is the same as filling a replay buffer withe the off-policy data and donÂ´t interakt with the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (878987, 1)\n",
      "obs (878987, 4)\n",
      "rewards (878987,)\n",
      "episode_returns (10000,)\n",
      "episode_starts (878987,)\n",
      "Total trajectories: -1\n",
      "Total transitions: 878987\n",
      "Average returns: 87.8987\n",
      "Std for returns: 5.305189752497077\n",
      "average reward: 87.54545454545455\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='cartpole_data.npz',\n",
    "                        traj_limitation=-1, batch_size=128)\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=0)\n",
    "# Pretrain the DQN\n",
    "model.pretrain(dataset, n_epochs=100)\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "reward_sum = 0.0\n",
    "episode_reward = []\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "#         Comment to render (rendering is not posible from remote)\n",
    "#         env.render()\n",
    "        if done:\n",
    "                episode_reward.append(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "print('average reward: {}'. format(np.mean(episode_reward)))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berkeley Artificial Intelligence Research (BAIR)\n",
    "Learn more about Berkeley Artificial Intelligence Research (BAIR) on [bair.berkeley.edu](https://bair.berkeley.edu/).\n",
    "\n",
    "### BEAR (Bootstrapping Error Accumulation Reduction)\n",
    "Papers:\n",
    "* Kumar et al. 2019 - [Stabilizing Off-Policy Q-learning via Bootstrapping Error Reduction](https://arxiv.org/abs/1906.00949)\n",
    "* Fu et al. 2020 - [Datasets for Data-Driven Reinforcement Learning](https://arxiv.org/abs/2004.07219)\n",
    "* Fujimoto et al 2018 - [Off-Policy Deep Reinforcement Learning without Exploration](https://arxiv.org/abs/1812.02900)\n",
    "\n",
    "Code:\n",
    "* BEAR on [GitHub](https://github.com/aviralkumar2907/BEAR) by Aviral Kumar\n",
    "* BCQ on [GitHub](https://github.com/sfujim/BCQ) by Scott Fujimoto\n",
    "\n",
    "Blog:\n",
    "* [Data-Driven Deep Reinforcement Learning](https://bair.berkeley.edu/blog/2019/12/05/bear/)\n",
    "\n",
    "Slides & Talks:\n",
    "* Stabilizing Off-Policy Q-learning via Bootstrapping Error Reduction - Introduction [Slides](https://sites.google.com/view/bear-off-policyrl)\n",
    "* Robust Perception, Imitation, and Reinforcement Learning for Embodied Learning Machines - [Talk](https://slideslive.com/38918103/robust-perception-imitation-and-reinforcement-learning-for-embodied-learning-machines?ref=speaker-17453-latest) by Sergey Levine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mujoco",
   "language": "python",
   "name": "venv_mujoco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
