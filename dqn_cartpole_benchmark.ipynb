{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banchmark fully off-policy DQN on Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is stable-baselines **Behavior Cloning** the right choice to make an off-policy dataset?\n",
    "* The dokumentation says: _\"for a given observation, the action taken by the policy must be the one taken by the expert\"_\n",
    "* That´s in my opption not the same as filling a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting off-policy data to tranin DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is saved as expert_cartpole.npz in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.gail import generate_expert_traj\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=0)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e6), n_episodes=int(1e4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning DQN with fully off-policy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I´m note sure if `model.pretrain()` is the same like a `reaply_buffer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (1054, 1)\n",
      "obs (1054, 4)\n",
      "rewards (1054,)\n",
      "episode_returns (100,)\n",
      "episode_starts (1054,)\n",
      "Total trajectories: -1\n",
      "Total transitions: 1054\n",
      "Average returns: 10.54\n",
      "Std for returns: 2.5472337937456784\n",
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n",
      "Pretraining with Behavior Cloning...\n",
      "==== Training progress 10.00% ====\n",
      "Epoch 10\n",
      "Training loss: 0.562942, Validation loss: 0.561478\n",
      "\n",
      "==== Training progress 20.00% ====\n",
      "Epoch 20\n",
      "Training loss: 0.400825, Validation loss: 0.408492\n",
      "\n",
      "==== Training progress 30.00% ====\n",
      "Epoch 30\n",
      "Training loss: 0.279523, Validation loss: 0.286960\n",
      "\n",
      "==== Training progress 40.00% ====\n",
      "Epoch 40\n",
      "Training loss: 0.215821, Validation loss: 0.228228\n",
      "\n",
      "==== Training progress 50.00% ====\n",
      "Epoch 50\n",
      "Training loss: 0.180851, Validation loss: 0.179090\n",
      "\n",
      "==== Training progress 60.00% ====\n",
      "Epoch 60\n",
      "Training loss: 0.157219, Validation loss: 0.157848\n",
      "\n",
      "==== Training progress 70.00% ====\n",
      "Epoch 70\n",
      "Training loss: 0.144404, Validation loss: 0.145440\n",
      "\n",
      "==== Training progress 80.00% ====\n",
      "Epoch 80\n",
      "Training loss: 0.134156, Validation loss: 0.137253\n",
      "\n",
      "==== Training progress 90.00% ====\n",
      "Epoch 90\n",
      "Training loss: 0.128554, Validation loss: 0.114727\n",
      "\n",
      "==== Training progress 100.00% ====\n",
      "Epoch 100\n",
      "Training loss: 0.121948, Validation loss: 0.110400\n",
      "\n",
      "Pretraining done.\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "11.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='expert_cartpole.npz',\n",
    "                        traj_limitation=-1, batch_size=128)\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "# Pretrain the DQN\n",
    "model.pretrain(dataset, n_epochs=100)\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "#         env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mujoco",
   "language": "python",
   "name": "venv_mujoco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
