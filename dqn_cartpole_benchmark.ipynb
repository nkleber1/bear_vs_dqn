{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banchmark fully off-policy DQN on Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataste\n",
    "Load `.npz` file and save data in variabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03360261,  0.04453236, -0.03984775,  0.01214404],\n",
       "       [-0.03271196, -0.14999613, -0.03960487,  0.29199302],\n",
       "       [-0.03571189, -0.34453166, -0.03376501,  0.5719267 ],\n",
       "       ...,\n",
       "       [ 1.8527644 ,  2.7364528 ,  0.20202377, -0.023985  ],\n",
       "       [ 1.9074935 ,  2.5390933 ,  0.20154408,  0.32502544],\n",
       "       [ 1.9582753 ,  2.730861  ,  0.20804459,  0.10205026]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('cartpole_data.npz')\n",
    "obs = data['obs'] # shape = (878987, 4)\n",
    "actions = data['actions'] # shape = (878987, 1)\n",
    "rewards = data['rewards'] # shape = (878987, )\n",
    "episode_starts = data['episode_starts']\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning DQN with fully off-policy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable-Baselines\n",
    "[Stable Baselines](https://stable-baselines.readthedocs.io/en/master/index.html) is a set of improved implementations of reinforcement learning algorithms based on OpenAI [Baselines](https://github.com/openai/baselines/).\n",
    "### Pre-Training (Behavior Cloning)\n",
    "With the `.pretrain()` method, you can pre-train RL policies using trajectories from an expert, and therefore accelerate training. [Learn more...](https://stable-baselines.readthedocs.io/en/master/guide/pretrain.html)\n",
    "* Stable-Baselines `.pretrain()` pretrains a model using behavior cloning: supervised learning given an expert dataset.\n",
    "* The dokumentation says: _\"for a given observation, the action taken by the policy must be the one taken by the expert\"_\n",
    "* Since Q-Learning has no Policy this is the same as filling a replay buffer withe the off-policy data and donÂ´t interakt with the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (878987, 1)\n",
      "obs (878987, 4)\n",
      "rewards (878987,)\n",
      "episode_returns (10000,)\n",
      "episode_starts (878987,)\n",
      "Total trajectories: -1\n",
      "Total transitions: 878987\n",
      "Average returns: 87.8987\n",
      "Std for returns: 5.305189752497077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method ExpertDataset.__del__ of <stable_baselines.gail.dataset.dataset.ExpertDataset object at 0x7f642fd4ec50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/atks/no_backup/envs/venv_mujoco/lib/python3.6/site-packages/stable_baselines/gail/dataset/dataset.py\", line 121, in __del__\n",
      "    del self.dataloader, self.train_loader, self.val_loader\n",
      "AttributeError: dataloader\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 91.5\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='cartpole_data.npz',\n",
    "                        traj_limitation=-1, batch_size=128)\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=0)\n",
    "# Pretrain the DQN\n",
    "model.pretrain(dataset, n_epochs=100)\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "reward_sum = 0.0\n",
    "episode_reward = []\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "#         Comment to render (rendering is not posible from remote)\n",
    "#         env.render()\n",
    "        if done:\n",
    "                episode_reward.append(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "print('average reward: {}'. format(np.mean(episode_reward)))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mujoco",
   "language": "python",
   "name": "venv_mujoco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
