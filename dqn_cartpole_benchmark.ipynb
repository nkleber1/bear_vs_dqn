{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banchmark fully off-policy DQN on Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is stable-baselines **Behavior Cloning** the right choice to make an off-policy dataset?\n",
    "* The dokumentation says: _\"for a given observation, the action taken by the policy must be the one taken by the expert\"_\n",
    "* That´s in my opption not the same as filling a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting off-policy data to tranin DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is saved as expert_cartpole.npz in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (878987, 1)\n",
      "obs (878987, 4)\n",
      "rewards (878987,)\n",
      "episode_returns (10000,)\n",
      "episode_starts (878987,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'actions': array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [1],\n",
       "        [1]]),\n",
       " 'obs': array([[-0.03360261,  0.04453236, -0.03984775,  0.01214404],\n",
       "        [-0.03271196, -0.14999613, -0.03960487,  0.29199302],\n",
       "        [-0.03571189, -0.34453166, -0.03376501,  0.5719267 ],\n",
       "        ...,\n",
       "        [ 1.8527644 ,  2.7364528 ,  0.20202377, -0.023985  ],\n",
       "        [ 1.9074935 ,  2.5390933 ,  0.20154408,  0.32502544],\n",
       "        [ 1.9582753 ,  2.730861  ,  0.20804459,  0.10205026]],\n",
       "       dtype=float32),\n",
       " 'rewards': array([1., 1., 1., ..., 1., 1., 1.]),\n",
       " 'episode_returns': array([91., 85., 85., ..., 88., 82., 92.]),\n",
       " 'episode_starts': array([ True, False, False, ..., False, False, False])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines.gail import generate_expert_traj\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=0)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e6), n_episodes=int(1e4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning DQN with fully off-policy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I´m note sure if `model.pretrain()` is the same like a `replay_buffer`\n",
    "* The Dokumentation says: _\"supervised learning given an expert dataset\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (878987, 1)\n",
      "obs (878987, 4)\n",
      "rewards (878987,)\n",
      "episode_returns (10000,)\n",
      "episode_starts (878987,)\n",
      "Total trajectories: -1\n",
      "Total transitions: 878987\n",
      "Average returns: 87.8987\n",
      "Std for returns: 5.305189752497077\n",
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n",
      "Pretraining with Behavior Cloning...\n",
      "==== Training progress 10.00% ====\n",
      "Epoch 10\n",
      "Training loss: 0.067389, Validation loss: 0.065586\n",
      "\n",
      "==== Training progress 20.00% ====\n",
      "Epoch 20\n",
      "Training loss: 0.052559, Validation loss: 0.056772\n",
      "\n",
      "==== Training progress 30.00% ====\n",
      "Epoch 30\n",
      "Training loss: 0.047303, Validation loss: 0.048568\n",
      "\n",
      "==== Training progress 40.00% ====\n",
      "Epoch 40\n",
      "Training loss: 0.044807, Validation loss: 0.044336\n",
      "\n",
      "==== Training progress 50.00% ====\n",
      "Epoch 50\n",
      "Training loss: 0.043372, Validation loss: 0.042358\n",
      "\n",
      "==== Training progress 60.00% ====\n",
      "Epoch 60\n",
      "Training loss: 0.042519, Validation loss: 0.041440\n",
      "\n",
      "==== Training progress 70.00% ====\n",
      "Epoch 70\n",
      "Training loss: 0.041752, Validation loss: 0.041401\n",
      "\n",
      "==== Training progress 80.00% ====\n",
      "Epoch 80\n",
      "Training loss: 0.041286, Validation loss: 0.040176\n",
      "\n",
      "==== Training progress 90.00% ====\n",
      "Epoch 90\n",
      "Training loss: 0.040791, Validation loss: 0.039665\n",
      "\n",
      "==== Training progress 100.00% ====\n",
      "Epoch 100\n",
      "Training loss: 0.040287, Validation loss: 0.039240\n",
      "\n",
      "Pretraining done.\n",
      "89.0\n",
      "95.0\n",
      "88.0\n",
      "89.0\n",
      "85.0\n",
      "94.0\n",
      "76.0\n",
      "79.0\n",
      "87.0\n",
      "94.0\n",
      "97.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='expert_cartpole.npz',\n",
    "                        traj_limitation=-1, batch_size=128)\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "# Pretrain the DQN\n",
    "model.pretrain(dataset, n_epochs=100)\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "#         env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('expert_cartpole.npz')\n",
    "DQN.replay_obs = data['obs'] # shape = (878987, 4)\n",
    "DQN.replay_actions = data['actions'] # shape = (878987, 1)\n",
    "DQN.replay_rewards = data['rewards'] # shape = (878987, )\n",
    "DQN.replay_episode_starts = data['episode_starts']\n",
    "\n",
    "def learn_from_buffer(self, total_timesteps, callback=None, log_interval=100, tb_log_name=\"DQN\",\n",
    "              reset_num_timesteps=True, replay_wrapper=None):\n",
    "\n",
    "        new_tb_log = self._init_num_timesteps(reset_num_timesteps)\n",
    "#         callback = self._init_callback(callback)\n",
    "\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name, new_tb_log) \\\n",
    "                as writer:\n",
    "            self._setup_learn()\n",
    "\n",
    "            # Create the replay buffer\n",
    "#             if self.prioritized_replay:\n",
    "#                 self.replay_buffer = PrioritizedReplayBuffer(self.buffer_size, alpha=self.prioritized_replay_alpha)\n",
    "#                 if self.prioritized_replay_beta_iters is None:\n",
    "#                     prioritized_replay_beta_iters = total_timesteps\n",
    "#                 else:\n",
    "#                     prioritized_replay_beta_iters = self.prioritized_replay_beta_iters\n",
    "#                 self.beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
    "#                                                     initial_p=self.prioritized_replay_beta0,\n",
    "#                                                     final_p=1.0)\n",
    "#             else:\n",
    "            self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "            self.beta_schedule = None\n",
    "            for i in range(len(episode_starts)-1):\n",
    "                if not episode_starts[i+1]:\n",
    "                    replay_buffer.add(self.replay_obs[i], self.replay_action[i], self.replay_reward[i],\n",
    "                                      self.replay_obs[i+1], self.replay_episode_starts[i+1])\n",
    "                else:\n",
    "                    replay_buffer.add(self.replay_obs[i], self.replay_action[i], self.replay_reward[i],\n",
    "                                      self.replay_obs[i+1], self.replay_episode_starts[i+1])\n",
    "\n",
    "#             if replay_wrapper is not None:\n",
    "#                 assert not self.prioritized_replay, \"Prioritized replay buffer is not supported by HER\"\n",
    "#                 self.replay_buffer = replay_wrapper(self.replay_buffer)\n",
    "\n",
    "            # Create the schedule for exploration starting from 1.\n",
    "#             self.exploration = LinearSchedule(schedule_timesteps=int(self.exploration_fraction * total_timesteps),\n",
    "#                                               initial_p=self.exploration_initial_eps,\n",
    "#                                               final_p=self.exploration_final_eps)\n",
    "\n",
    "            episode_rewards = [0.0]\n",
    "            episode_successes = []\n",
    "\n",
    "#             callback.on_training_start(locals(), globals())\n",
    "#             callback.on_rollout_start()\n",
    "\n",
    "            reset = True\n",
    "            obs = self.env.reset()\n",
    "            # Retrieve unnormalized observation for saving into the buffer\n",
    "#             if self._vec_normalize_env is not None:\n",
    "#                 obs_ = self._vec_normalize_env.get_original_obs().squeeze()\n",
    "\n",
    "            for _ in range(total_timesteps):\n",
    "                # Take action and update exploration to the newest value\n",
    "                kwargs = {}\n",
    "                if not self.param_noise:\n",
    "                    update_eps = self.exploration.value(self.num_timesteps)\n",
    "                    update_param_noise_threshold = 0.\n",
    "                else:\n",
    "                    update_eps = 0.\n",
    "                    # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
    "                    # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
    "                    # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
    "                    # for detailed explanation.\n",
    "                    update_param_noise_threshold = \\\n",
    "                        -np.log(1. - self.exploration.value(self.num_timesteps) +\n",
    "                                self.exploration.value(self.num_timesteps) / float(self.env.action_space.n))\n",
    "                    kwargs['reset'] = reset\n",
    "                    kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
    "                    kwargs['update_param_noise_scale'] = True\n",
    "#                 with self.sess.as_default():\n",
    "#                     action = self.act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
    "#                 env_action = action\n",
    "#                 reset = False\n",
    "#                 new_obs, rew, done, info = self.env.step(env_action)\n",
    "\n",
    "                self.num_timesteps += 1\n",
    "\n",
    "#                 # Stop training if return value is False\n",
    "#                 if callback.on_step() is False:\n",
    "#                     break\n",
    "\n",
    "#                 # Store only the unnormalized version\n",
    "#                 if self._vec_normalize_env is not None:\n",
    "#                     new_obs_ = self._vec_normalize_env.get_original_obs().squeeze()\n",
    "#                     reward_ = self._vec_normalize_env.get_original_reward().squeeze()\n",
    "#                 else:\n",
    "#                     # Avoid changing the original ones\n",
    "#                     obs_, new_obs_, reward_ = obs, new_obs, rew\n",
    "#                 # Store transition in the replay buffer.\n",
    "#                 self.replay_buffer.add(obs_, action, reward_, new_obs_, float(done))\n",
    "#                 obs = new_obs\n",
    "#                 # Save the unnormalized observation\n",
    "#                 if self._vec_normalize_env is not None:\n",
    "#                     obs_ = new_obs_\n",
    "\n",
    "                if writer is not None:\n",
    "                    ep_rew = np.array([reward_]).reshape((1, -1))\n",
    "                    ep_done = np.array([done]).reshape((1, -1))\n",
    "                    tf_util.total_episode_reward_logger(self.episode_reward, ep_rew, ep_done, writer,\n",
    "                                                        self.num_timesteps)\n",
    "\n",
    "#                 episode_rewards[-1] += reward_\n",
    "#                 if done:\n",
    "#                     maybe_is_success = info.get('is_success')\n",
    "#                     if maybe_is_success is not None:\n",
    "#                         episode_successes.append(float(maybe_is_success))\n",
    "#                     if not isinstance(self.env, VecEnv):\n",
    "#                         obs = self.env.reset()\n",
    "#                     episode_rewards.append(0.0)\n",
    "                    reset = True\n",
    "\n",
    "                # Do not train if the warmup phase is not over\n",
    "                # or if there are not enough samples in the replay buffer\n",
    "                can_sample = self.replay_buffer.can_sample(self.batch_size)\n",
    "#                 if can_sample and self.num_timesteps > self.learning_starts \\\n",
    "#                         and self.num_timesteps % self.train_freq == 0:\n",
    "\n",
    "#                 callback.on_rollout_end()\n",
    "#                 # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "#                 # pytype:disable=bad-unpacking\n",
    "                if self.prioritized_replay:\n",
    "                    assert self.beta_schedule is not None, \\\n",
    "                           \"BUG: should be LinearSchedule when self.prioritized_replay True\"\n",
    "                    experience = self.replay_buffer.sample(self.batch_size,\n",
    "                                                           beta=self.beta_schedule.value(self.num_timesteps),\n",
    "                                                           env=self._vec_normalize_env)\n",
    "                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
    "                else:\n",
    "                    obses_t, actions, rewards, obses_tp1, dones = self.replay_buffer.sample(self.batch_size,\n",
    "                                                                                            env=self._vec_normalize_env)\n",
    "                    weights, batch_idxes = np.ones_like(rewards), None\n",
    "                # pytype:enable=bad-unpacking\n",
    "\n",
    "                if writer is not None:\n",
    "                    # run loss backprop with summary, but once every 100 steps save the metadata\n",
    "                    # (memory, compute time, ...)\n",
    "                    if (1 + self.num_timesteps) % 100 == 0:\n",
    "                        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                        run_metadata = tf.RunMetadata()\n",
    "                        summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                              dones, weights, sess=self.sess, options=run_options,\n",
    "                                                              run_metadata=run_metadata)\n",
    "                        writer.add_run_metadata(run_metadata, 'step%d' % self.num_timesteps)\n",
    "                    else:\n",
    "                        summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                              dones, weights, sess=self.sess)\n",
    "                    writer.add_summary(summary, self.num_timesteps)\n",
    "                else:\n",
    "                    _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights,\n",
    "                                                    sess=self.sess)\n",
    "\n",
    "                if self.prioritized_replay:\n",
    "                    new_priorities = np.abs(td_errors) + self.prioritized_replay_eps\n",
    "                    assert isinstance(self.replay_buffer, PrioritizedReplayBuffer)\n",
    "                    self.replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "\n",
    "#                 callback.on_rollout_start()\n",
    "\n",
    "#                 if can_sample and self.num_timesteps > self.learning_starts and \\\n",
    "#                         self.num_timesteps % self.target_network_update_freq == 0:\n",
    "#                     # Update target network periodically.\n",
    "#                     self.update_target(sess=self.sess)\n",
    "\n",
    "#                 if len(episode_rewards[-101:-1]) == 0:\n",
    "#                     mean_100ep_reward = -np.inf\n",
    "#                 else:\n",
    "#                     mean_100ep_reward = round(float(np.mean(episode_rewards[-101:-1])), 1)\n",
    "\n",
    "#                 num_episodes = len(episode_rewards)\n",
    "#                 if self.verbose >= 1 and done and log_interval is not None and len(episode_rewards) % log_interval == 0:\n",
    "#                     logger.record_tabular(\"steps\", self.num_timesteps)\n",
    "#                     logger.record_tabular(\"episodes\", num_episodes)\n",
    "#                     if len(episode_successes) > 0:\n",
    "#                         logger.logkv(\"success rate\", np.mean(episode_successes[-100:]))\n",
    "#                     logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
    "#                     logger.record_tabular(\"% time spent exploring\",\n",
    "#                                           int(100 * self.exploration.value(self.num_timesteps)))\n",
    "#                     logger.dump_tabular()\n",
    "\n",
    "#         callback.on_training_end()\n",
    "        return self\n",
    "\n",
    "DQN.learn = learn_from_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03360261,  0.04453236, -0.03984775,  0.01214404],\n",
       "       [-0.03271196, -0.14999613, -0.03960487,  0.29199302],\n",
       "       [-0.03571189, -0.34453166, -0.03376501,  0.5719267 ],\n",
       "       ...,\n",
       "       [ 1.8527644 ,  2.7364528 ,  0.20202377, -0.023985  ],\n",
       "       [ 1.9074935 ,  2.5390933 ,  0.20154408,  0.32502544],\n",
       "       [ 1.9582753 ,  2.730861  ,  0.20804459,  0.10205026]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('expert_cartpole.npz')\n",
    "obs = data['obs'] # shape = (878987, 4)\n",
    "actions = data['actions'] # shape = (878987, 1)\n",
    "rewards = data['rewards'] # shape = (878987, )\n",
    "episode_starts = data['episode_starts']\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=1, buffer_size=(len(episode_starts)-1))\n",
    "\n",
    "model.replay_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines.common.segment_tree import SumSegmentTree, MinSegmentTree\n",
    "from stable_baselines.common.vec_env import VecNormalize\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size: int):\n",
    "        \"\"\"\n",
    "        Implements a ring buffer (FIFO).\n",
    "        :param size: (int)  Max number of transitions to store in the buffer. When the buffer overflows the old\n",
    "            memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._storage)\n",
    "\n",
    "    @property\n",
    "    def storage(self):\n",
    "        \"\"\"[(Union[np.ndarray, int], Union[np.ndarray, int], float, Union[np.ndarray, int], bool)]: content of the replay buffer\"\"\"\n",
    "        return self._storage\n",
    "\n",
    "    @property\n",
    "    def buffer_size(self) -> int:\n",
    "        \"\"\"float: Max capacity of the buffer\"\"\"\n",
    "        return self._maxsize\n",
    "\n",
    "    def can_sample(self, n_samples: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check if n_samples samples can be sampled\n",
    "        from the buffer.\n",
    "        :param n_samples: (int)\n",
    "        :return: (bool)\n",
    "        \"\"\"\n",
    "        return len(self) >= n_samples\n",
    "\n",
    "    def is_full(self) -> int:\n",
    "        \"\"\"\n",
    "        Check whether the replay buffer is full or not.\n",
    "        :return: (bool)\n",
    "        \"\"\"\n",
    "        return len(self) == self.buffer_size\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        \"\"\"\n",
    "        add a new transition to the buffer\n",
    "        :param obs_t: (Union[np.ndarray, int]) the last observation\n",
    "        :param action: (Union[np.ndarray, int]) the action\n",
    "        :param reward: (float) the reward of the transition\n",
    "        :param obs_tp1: (Union[np.ndarray, int]) the current observation\n",
    "        :param done: (bool) is the episode done\n",
    "        \"\"\"\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def extend(self, obs_t, action, reward, obs_tp1, done):\n",
    "        \"\"\"\n",
    "        add a new batch of transitions to the buffer\n",
    "        :param obs_t: (Union[Tuple[Union[np.ndarray, int]], np.ndarray]) the last batch of observations\n",
    "        :param action: (Union[Tuple[Union[np.ndarray, int]]], np.ndarray]) the batch of actions\n",
    "        :param reward: (Union[Tuple[float], np.ndarray]) the batch of the rewards of the transition\n",
    "        :param obs_tp1: (Union[Tuple[Union[np.ndarray, int]], np.ndarray]) the current batch of observations\n",
    "        :param done: (Union[Tuple[bool], np.ndarray]) terminal status of the batch\n",
    "        Note: uses the same names as .add to keep compatibility with named argument passing\n",
    "                but expects iterables and arrays with more than 1 dimensions\n",
    "        \"\"\"\n",
    "        for data in zip(obs_t, action, reward, obs_tp1, done):\n",
    "            if self._next_idx >= len(self._storage):\n",
    "                self._storage.append(data)\n",
    "            else:\n",
    "                self._storage[self._next_idx] = data\n",
    "            self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_obs(obs: np.ndarray,\n",
    "                       env: Optional[VecNormalize] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper for normalizing the observation.\n",
    "        \"\"\"\n",
    "        if env is not None:\n",
    "            return env.normalize_obs(obs)\n",
    "        return obs\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_reward(reward: np.ndarray,\n",
    "                          env: Optional[VecNormalize] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper for normalizing the reward.\n",
    "        \"\"\"\n",
    "        if env is not None:\n",
    "            return env.normalize_reward(reward)\n",
    "        return reward\n",
    "\n",
    "    def _encode_sample(self, idxes: Union[List[int], np.ndarray], env: Optional[VecNormalize] = None):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return (self._normalize_obs(np.array(obses_t), env),\n",
    "                np.array(actions),\n",
    "                self._normalize_reward(np.array(rewards), env),\n",
    "                self._normalize_obs(np.array(obses_tp1), env),\n",
    "                np.array(dones))\n",
    "\n",
    "    def sample(self, batch_size: int, env: Optional[VecNormalize] = None, **_kwargs):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences.\n",
    "        :param batch_size: (int) How many transitions to sample.\n",
    "        :param env: (Optional[VecNormalize]) associated gym VecEnv\n",
    "            to normalize the observations/rewards when sampling\n",
    "        :return:\n",
    "            - obs_batch: (np.ndarray) batch of observations\n",
    "            - act_batch: (numpy float) batch of actions executed given obs_batch\n",
    "            - rew_batch: (numpy float) rewards received as results of executing act_batch\n",
    "            - next_obs_batch: (np.ndarray) next set of observations seen after executing act_batch\n",
    "            - done_mask: (numpy bool) done_mask[i] = 1 if executing act_batch[i] resulted in the end of an episode\n",
    "                and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-132-9d73955afd75>, line 56)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-132-9d73955afd75>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    update_param_noise_threshold = \\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class OOP_DQN(DQN):\n",
    "    def __init__(self, npz_path, *args, **kwargs):\n",
    "        super(OOP_DQN, self).__init__(*args, **kwargs)\n",
    "        print(npz_path)\n",
    "        data = np.load(npz_path)\n",
    "        self.replay_obs = data['obs'] # shape = (878987, 4)\n",
    "        self.replay_actions = data['actions'] # shape = (878987, 1)\n",
    "        self.replay_rewards = data['rewards'] # shape = (878987, )\n",
    "        self.replay_episode_starts = data['episode_starts']\n",
    "              \n",
    "    def learn_from_buffer(self, total_timesteps, callback=None, log_interval=100, tb_log_name=\"DQN\",\n",
    "              reset_num_timesteps=True, replay_wrapper=None):\n",
    "\n",
    "        new_tb_log = self._init_num_timesteps(reset_num_timesteps)\n",
    "#         callback = self._init_callback(callback)\n",
    "\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name, new_tb_log) \\\n",
    "                as writer:\n",
    "            self._setup_learn()\n",
    "\n",
    "            # Create the replay buffer\n",
    "            self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "            self.beta_schedule = None\n",
    "            for i in range(len(episode_starts)-1):\n",
    "                if not episode_starts[i+1]:\n",
    "                    self.replay_buffer.add(self.replay_obs[i], self.replay_actions[i], self.replay_rewards[i],\n",
    "                                           self.replay_obs[i+1], self.replay_episode_starts[i+1])\n",
    "                else:\n",
    "                    self.replay_buffer.add(self.replay_obs[i], self.replay_actions[i], self.replay_rewards[i],\n",
    "                                           self.replay_obs[i+1], self.replay_episode_starts[i+1])\n",
    "\n",
    "            episode_rewards = [0.0]\n",
    "            episode_successes = []\n",
    "\n",
    "#             callback.on_training_start(locals(), globals())\n",
    "#             callback.on_rollout_start()\n",
    "\n",
    "            reset = True\n",
    "            obs = self.env.reset()\n",
    "#             # Retrieve unnormalized observation for saving into the buffer\n",
    "#             if self._vec_normalize_env is not None:\n",
    "#                 obs_ = self._vec_normalize_env.get_original_obs().squeeze()\n",
    "\n",
    "            for _ in range(total_timesteps):\n",
    "                # Take action and update exploration to the newest value\n",
    "                kwargs = {}\n",
    "#                 if not self.param_noise:\n",
    "#                     update_eps = self.exploration.value(self.num_timesteps)\n",
    "#                     update_param_noise_threshold = 0.\n",
    "#                 else:\n",
    "#                     update_eps = 0.\n",
    "                    # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
    "                    # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
    "                    # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
    "                    # for detailed explanation.\n",
    "                    update_param_noise_threshold = \\\n",
    "                        -np.log(1. - self.exploration.value(self.num_timesteps) +\n",
    "                                self.exploration.value(self.num_timesteps) / float(self.env.action_space.n))\n",
    "                    kwargs['reset'] = reset\n",
    "                    kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
    "                    kwargs['update_param_noise_scale'] = True\n",
    "#                 with self.sess.as_default():\n",
    "#                     action = self.act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
    "#                 env_action = action\n",
    "#                 reset = False\n",
    "#                 new_obs, rew, done, info = self.env.step(env_action)\n",
    "\n",
    "                self.num_timesteps += 1\n",
    "\n",
    "#                 # Stop training if return value is False\n",
    "#                 if callback.on_step() is False:\n",
    "#                     break\n",
    "\n",
    "#                 # Store only the unnormalized version\n",
    "#                 if self._vec_normalize_env is not None:\n",
    "#                     new_obs_ = self._vec_normalize_env.get_original_obs().squeeze()\n",
    "#                     reward_ = self._vec_normalize_env.get_original_reward().squeeze()\n",
    "#                 else:\n",
    "#                     # Avoid changing the original ones\n",
    "#                     obs_, new_obs_, reward_ = obs, new_obs, rew\n",
    "#                 # Store transition in the replay buffer.\n",
    "#                 self.replay_buffer.add(obs_, action, reward_, new_obs_, float(done))\n",
    "#                 obs = new_obs\n",
    "#                 # Save the unnormalized observation\n",
    "#                 if self._vec_normalize_env is not None:\n",
    "#                     obs_ = new_obs_\n",
    "\n",
    "                if writer is not None:\n",
    "                    ep_rew = np.array([reward_]).reshape((1, -1))\n",
    "                    ep_done = np.array([done]).reshape((1, -1))\n",
    "                    tf_util.total_episode_reward_logger(self.episode_reward, ep_rew, ep_done, writer,\n",
    "                                                        self.num_timesteps)\n",
    "\n",
    "#                 episode_rewards[-1] += reward_\n",
    "#                 if done:\n",
    "#                     maybe_is_success = info.get('is_success')\n",
    "#                     if maybe_is_success is not None:\n",
    "#                         episode_successes.append(float(maybe_is_success))\n",
    "#                     if not isinstance(self.env, VecEnv):\n",
    "#                         obs = self.env.reset()\n",
    "#                     episode_rewards.append(0.0)\n",
    "                    reset = True\n",
    "\n",
    "                # Do not train if the warmup phase is not over\n",
    "                # or if there are not enough samples in the replay buffer\n",
    "                can_sample = self.replay_buffer.can_sample(self.batch_size)\n",
    "#                 if can_sample and self.num_timesteps > self.learning_starts \\\n",
    "#                         and self.num_timesteps % self.train_freq == 0:\n",
    "\n",
    "#                 callback.on_rollout_end()\n",
    "                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "                # pytype:disable=bad-unpacking\n",
    "                obses_t, actions, rewards, obses_tp1, dones = self.replay_buffer.sample(self.batch_size,\n",
    "                                                                                        env=self._vec_normalize_env)\n",
    "                weights, batch_idxes = np.ones_like(rewards), None\n",
    "                # pytype:enable=bad-unpacking\n",
    "\n",
    "                if writer is not None:\n",
    "                    # run loss backprop with summary, but once every 100 steps save the metadata\n",
    "                    # (memory, compute time, ...)\n",
    "                    if (1 + self.num_timesteps) % 100 == 0:\n",
    "                        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                        run_metadata = tf.RunMetadata()\n",
    "                        summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                              dones, weights, sess=self.sess, options=run_options,\n",
    "                                                              run_metadata=run_metadata)\n",
    "                        writer.add_run_metadata(run_metadata, 'step%d' % self.num_timesteps)\n",
    "                    else:\n",
    "                        summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                              dones, weights, sess=self.sess)\n",
    "                    writer.add_summary(summary, self.num_timesteps)\n",
    "                else:\n",
    "                    _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights,\n",
    "                                                    sess=self.sess)\n",
    "\n",
    "                if self.prioritized_replay:\n",
    "                    new_priorities = np.abs(td_errors) + self.prioritized_replay_eps\n",
    "                    assert isinstance(self.replay_buffer, PrioritizedReplayBuffer)\n",
    "                    self.replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "\n",
    "#                 callback.on_rollout_start()\n",
    "\n",
    "#                 if can_sample and self.num_timesteps > self.learning_starts and \\\n",
    "#                         self.num_timesteps % self.target_network_update_freq == 0:\n",
    "#                     # Update target network periodically.\n",
    "#                     self.update_target(sess=self.sess)\n",
    "\n",
    "#                 if len(episode_rewards[-101:-1]) == 0:\n",
    "#                     mean_100ep_reward = -np.inf\n",
    "#                 else:\n",
    "#                     mean_100ep_reward = round(float(np.mean(episode_rewards[-101:-1])), 1)\n",
    "\n",
    "#                 num_episodes = len(episode_rewards)\n",
    "#                 if self.verbose >= 1 and done and log_interval is not None and len(episode_rewards) % log_interval == 0:\n",
    "#                     logger.record_tabular(\"steps\", self.num_timesteps)\n",
    "#                     logger.record_tabular(\"episodes\", num_episodes)\n",
    "#                     if len(episode_successes) > 0:\n",
    "#                         logger.logkv(\"success rate\", np.mean(episode_successes[-100:]))\n",
    "#                     logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
    "#                     logger.record_tabular(\"% time spent exploring\",\n",
    "#                                           int(100 * self.exploration.value(self.num_timesteps)))\n",
    "#                     logger.dump_tabular()\n",
    "\n",
    "#         callback.on_training_end()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating environment from the given name, wrapped in a DummyVecEnv.\n",
      "expert_cartpole.npz\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-3863c4968048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOOP_DQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expert_cartpole.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_starts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_from_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-89ef518992d4>\u001b[0m in \u001b[0;36mlearn_from_buffer\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_noise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mupdate_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0mupdate_param_noise_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "model = OOP_DQN('expert_cartpole.npz', 'MlpPolicy', 'CartPole-v1', verbose=1, buffer_size=(len(episode_starts)-1))\n",
    "model.learn_from_buffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mujoco",
   "language": "python",
   "name": "venv_mujoco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
