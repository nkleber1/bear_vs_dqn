{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring BEAR Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "* __TODO__ Check why cuda is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is NOT available\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as td\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Cuda is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Cuda is NOT available\")\n",
    "\n",
    "# Intern Imports\n",
    "import BEAR.algos as algos\n",
    "from BEAR.logger import logger, setup_logger\n",
    "from BEAR.logger import create_stats_ordered_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "* __TODO__ Explore step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim=10, action_dim=4, size=1000000):\n",
    "        self.storage = dict()\n",
    "        self.storage['observations'] = np.zeros((size, state_dim), np.float32)\n",
    "        self.storage['next_observations'] = np.zeros((size, state_dim), np.float32)\n",
    "        self.storage['actions'] = np.zeros((size, action_dim), np.float32)\n",
    "        self.storage['rewards'] = np.zeros((size, 1), np.float32)\n",
    "        self.storage['terminals'] = np.zeros((size, 1), np.float32)\n",
    "        self.storage['bootstrap_mask'] = np.zeros((size, 4), np.float32)\n",
    "        self.buffer_size = size\n",
    "        self.ctr = 0\n",
    "\n",
    "    # Expects tuples of (state, next_state, action, reward, done)\n",
    "    def add(self, data):\n",
    "        self.storage['observations'][self.ctr] = data[0]\n",
    "        self.storage['next_observations'][self.ctr] = data[1]\n",
    "        self.storage['actions'][self.ctr] = data[2]\n",
    "        self.storage['rewards'][self.ctr] = data[3]\n",
    "        self.storage['terminals'][self.ctr] = data[4]\n",
    "        self.ctr += 1\n",
    "        self.ctr = self.ctr % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size, with_data_policy=False):\n",
    "        ind = np.random.randint(0, self.storage['observations'].shape[0], size=batch_size)\n",
    "        state, next_state, action, reward, done = [], [], [], [], []\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        s = self.storage['observations'][ind]\n",
    "        a = self.storage['actions'][ind]\n",
    "        r = self.storage['rewards'][ind]\n",
    "        s2 = self.storage['next_observations'][ind]\n",
    "        d = self.storage['terminals'][ind]\n",
    "        mask = self.storage['bootstrap_mask'][ind]\n",
    "\n",
    "        if with_data_policy:\n",
    "                data_mean = self.storage['data_policy_mean'][ind]\n",
    "                data_cov = self.storage['data_policy_logvar'][ind]\n",
    "\n",
    "                return (np.array(s), \n",
    "                        np.array(s2), \n",
    "                        np.array(a), \n",
    "                        np.array(r).reshape(-1, 1), \n",
    "                        np.array(d).reshape(-1, 1),\n",
    "                        np.array(mask),\n",
    "                        np.array(data_mean),\n",
    "                        np.array(data_cov))\n",
    "\n",
    "        return (np.array(s), \n",
    "                np.array(s2), \n",
    "                np.array(a), \n",
    "                np.array(r).reshape(-1, 1), \n",
    "                np.array(d).reshape(-1, 1),\n",
    "                np.array(mask))\n",
    "\n",
    "    def save(self, path):\n",
    "        np.save(path+\".npy\", self.storage)\n",
    "\n",
    "    def load(self, filename, bootstrap_dim=None):\n",
    "#         with gzip.open(filename, 'rb') as f:\n",
    "#                 self.storage = pickle.load(f)\n",
    "                \n",
    "#         with open(filename, 'rb') as f:\n",
    "#                self.storage = pickle.load(f)\n",
    "        keys = ('observations', 'next_observations', 'actions', 'rewards', 'terminals', 'bootstrap_mask')\n",
    "        storage = np.load(filename, allow_pickle=True).item()\n",
    "        self.storage = dict()\n",
    "        for key in storage.keys():\n",
    "              self.storage[key] = storage[key]\n",
    "                \n",
    "        sum_returns = self.storage['rewards'].sum()\n",
    "        num_traj = self.storage['terminals'].sum()\n",
    "        if num_traj == 0:\n",
    "                num_traj = 1000\n",
    "        average_per_traj_return = sum_returns/num_traj\n",
    "        print (\"Average Return: \", average_per_traj_return)\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        num_samples = self.storage['observations'].shape[0]\n",
    "        if bootstrap_dim is not None:\n",
    "                self.bootstrap_dim = bootstrap_dim\n",
    "                bootstrap_mask = np.random.binomial(n=1, size=(1, num_samples, bootstrap_dim,), p=0.8)\n",
    "                bootstrap_mask = np.squeeze(bootstrap_mask, axis=0)\n",
    "                self.storage['bootstrap_mask'] = bootstrap_mask[:num_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEAR\n",
    "* __Q__ Was macht der Actor?\n",
    "* __Q__ Was macht der Critic? Warum berechnet er 2 bzw. 4 Qs? Warum hat der Critic eine `with_var`? \n",
    "* __Q__ Warum wird ein VAE benötigt? Für __BC__? Warum wird ist der Decoder input die latente Represnetation und die Observation?\n",
    "* __Q__ Was passiert wenn `mode='auto'`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEAR(object):\n",
    "    def __init__(self, num_qs, state_dim, action_dim, max_action, delta_conf=0.1, use_bootstrap=True, version=0, lambda_=0.4,\n",
    "                 threshold=0.05, mode='auto', num_samples_match=10, mmd_sigma=10.0,\n",
    "                 lagrange_thresh=10.0, use_kl=False, use_ensemble=True, kernel_type='laplacian'):\n",
    "        \n",
    "        \n",
    "        \"\"\" \n",
    "        Setup Actor\n",
    "        # (l1): ReLu - Linear(state_dim, 400)\n",
    "        # (l2): ReLu - Linear(400, 300)\n",
    "        # (mean / log_std): Linear(300, action_dim) / Linear(300, action_dim)\n",
    "        # (z) mean + std * randomTensor --> Sampalning from learnd Distribution (reparameterization trick)\n",
    "        # (return) max_action * torch.tanh(z) --> ??? \n",
    "        \"\"\"\n",
    "        self.actor = algos.RegularActor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = algos.RegularActor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "\n",
    "        \n",
    "        \"\"\" \n",
    "        Setup Critic\n",
    "        # num_qs is never used\n",
    "        # For all Qs: \n",
    "        #    (l1): ReLu - Linear(state_dim + action_dim, 400)\n",
    "        #    (l2): ReLu - Linear(400, 300)\n",
    "        #    (l3): ReLu - Linear(300, 1)\n",
    "        # return 2 or 4 q-values\n",
    "        \"\"\" \n",
    "        self.critic = algos.EnsembleCritic(num_qs, state_dim, action_dim).to(device)\n",
    "        self.critic_target = algos.EnsembleCritic(num_qs, state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        \n",
    "        \n",
    "        \"\"\" \n",
    "        Setup VAE (Vanilla Variational Auto-Encoder)\n",
    "        # Encoder\n",
    "        #   (e1): Linear(state_dim + action_dim, 750)\n",
    "        #   (e2): Linear(750, 750)\n",
    "        # Sampel z (reparameterization trick)\n",
    "        #   (mean): Linear(in_features=750, latent_dim)\n",
    "        #   (log_std): Linear(in_features=750, latent_dim)\n",
    "        #   (z) mean + std * randomTensor\n",
    "        # Decoder\n",
    "        #   (d1): Linear(state_dim + latent_dim, 750) --> Why is latent + state(obs)\n",
    "        #   (d2): Linear(750, 750)\n",
    "        #   (d3): Linear(750, action_dim)\n",
    "        \"\"\" \n",
    "        latent_dim = action_dim * 2\n",
    "        self.vae = algos.VAE(state_dim, action_dim, latent_dim, max_action).to(device)\n",
    "        self.vae_optimizer = torch.optim.Adam(self.vae.parameters()) \n",
    "\n",
    "        \"\"\" \n",
    "        Save args in self\n",
    "        \"\"\" \n",
    "        self.max_action = max_action\n",
    "        self.action_dim = action_dim\n",
    "        self.delta_conf = delta_conf\n",
    "        self.use_bootstrap = use_bootstrap\n",
    "        self.version = version\n",
    "        self._lambda = lambda_\n",
    "        self.threshold = threshold\n",
    "        self.mode = mode\n",
    "        self.num_qs = num_qs\n",
    "        self.num_samples_match = num_samples_match\n",
    "        self.mmd_sigma = mmd_sigma\n",
    "        self.lagrange_thresh = lagrange_thresh\n",
    "        self.use_kl = use_kl\n",
    "        self.use_ensemble = use_ensemble\n",
    "        self.kernel_type = kernel_type\n",
    "        \n",
    "    ### Karam Fragen was hier passiert ### \n",
    "        if self.mode == 'auto':\n",
    "            # Use lagrange multipliers on the constraint if set to auto mode \n",
    "            # for the purpose of maintaing support matching at all times\n",
    "            self.log_lagrange2 = torch.randn((), requires_grad=True, device=device)\n",
    "            self.lagrange2_opt = torch.optim.Adam([self.log_lagrange2,], lr=1e-3)\n",
    "\n",
    "        self.epoch = 0\n",
    "\n",
    "    #MMD distance between actions as a measure of support divergence.\n",
    "    def mmd_loss_laplacian(self, samples1, samples2, sigma=0.2):\n",
    "        \"\"\"MMD constraint with Laplacian kernel for support matching\"\"\"\n",
    "        # sigma is set to 10.0 for hopper, cheetah and 20 for walker/ant\n",
    "        diff_x_x = samples1.unsqueeze(2) - samples1.unsqueeze(1)  # B x N x N x d\n",
    "        diff_x_x = torch.mean((-(diff_x_x.abs()).sum(-1)/(2.0 * sigma)).exp(), dim=(1,2))\n",
    "\n",
    "        diff_x_y = samples1.unsqueeze(2) - samples2.unsqueeze(1)\n",
    "        diff_x_y = torch.mean((-(diff_x_y.abs()).sum(-1)/(2.0 * sigma)).exp(), dim=(1, 2))\n",
    "\n",
    "        diff_y_y = samples2.unsqueeze(2) - samples2.unsqueeze(1)  # B x N x N x d\n",
    "        diff_y_y = torch.mean((-(diff_y_y.abs()).sum(-1)/(2.0 * sigma)).exp(), dim=(1,2))\n",
    "\n",
    "        overall_loss = (diff_x_x + diff_y_y - 2.0 * diff_x_y + 1e-6).sqrt()\n",
    "        return overall_loss\n",
    "    \n",
    "    def mmd_loss_gaussian(self, samples1, samples2, sigma=0.2):\n",
    "        \"\"\"MMD constraint with Gaussian Kernel support matching\"\"\"\n",
    "        # sigma is set to 10.0 for hopper, cheetah and 20 for walker/ant\n",
    "        diff_x_x = samples1.unsqueeze(2) - samples1.unsqueeze(1)  # B x N x N x d\n",
    "        diff_x_x = torch.mean((-(diff_x_x.pow(2)).sum(-1)/(2.0 * sigma)).exp(), dim=(1,2))\n",
    "\n",
    "        diff_x_y = samples1.unsqueeze(2) - samples2.unsqueeze(1)\n",
    "        diff_x_y = torch.mean((-(diff_x_y.pow(2)).sum(-1)/(2.0 * sigma)).exp(), dim=(1, 2))\n",
    "\n",
    "        diff_y_y = samples2.unsqueeze(2) - samples2.unsqueeze(1)  # B x N x N x d\n",
    "        diff_y_y = torch.mean((-(diff_y_y.pow(2)).sum(-1)/(2.0 * sigma)).exp(), dim=(1,2))\n",
    "\n",
    "        overall_loss = (diff_x_x + diff_y_y - 2.0 * diff_x_y + 1e-6).sqrt()\n",
    "        return overall_loss\n",
    "\n",
    "    def kl_loss(self, samples1, state, sigma=0.2):\n",
    "        \"\"\"We just do likelihood, we make sure that the policy is close to the\n",
    "           data in terms of the KL.\"\"\"\n",
    "        state_rep = state.unsqueeze(1).repeat(1, samples1.size(1), 1).view(-1, state.size(-1))\n",
    "        samples1_reshape = samples1.view(-1, samples1.size(-1))\n",
    "        samples1_log_pis = self.actor.log_pis(state=state_rep, raw_action=samples1_reshape)\n",
    "        samples1_log_prob = samples1_log_pis.view(state.size(0), samples1.size(1))\n",
    "        return (-samples1_log_prob).mean(1)\n",
    "    \n",
    "    def entropy_loss(self, samples1, state, sigma=0.2):\n",
    "        state_rep = state.unsqueeze(1).repeat(1, samples1.size(1), 1).view(-1, state.size(-1))\n",
    "        samples1_reshape = samples1.view(-1, samples1.size(-1))\n",
    "        samples1_log_pis = self.actor.log_pis(state=state_rep, raw_action=samples1_reshape)\n",
    "        samples1_log_prob = samples1_log_pis.view(state.size(0), samples1.size(1))\n",
    "        # print (samples1_log_prob.min(), samples1_log_prob.max())\n",
    "        samples1_prob = samples1_log_prob.clamp(min=-5, max=4).exp()\n",
    "        return (samples1_prob).mean(1)\n",
    "    \n",
    "    def select_action(self, state):      \n",
    "        \"\"\"When running the actor, we just select action based on the max of the Q-function computed over\n",
    "            samples from the policy -- which biases things to support.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state.reshape(1, -1)).repeat(10, 1).to(device)\n",
    "            action = self.actor(state)                       # sampels 10 actions\n",
    "            q1 = self.critic.q1(state, action)               # returns a Q-Value for evry sampeld action \n",
    "            ind = q1.max(0)[1]                               # use action withe highest Q-Velue\n",
    "        return action[ind].cpu().data.numpy().flatten()\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005):\n",
    "        for it in range(iterations):\n",
    "            \n",
    "            \n",
    "            # sample a batch of sasr and transform them from numpy to tensor\n",
    "            state_np, next_state_np, action, reward, done, mask = replay_buffer.sample(batch_size)\n",
    "            state           = torch.FloatTensor(state_np).to(device)\n",
    "            action          = torch.FloatTensor(action).to(device)\n",
    "            next_state      = torch.FloatTensor(next_state_np).to(device)\n",
    "            reward          = torch.FloatTensor(reward).to(device)\n",
    "            done            = torch.FloatTensor(1 - done).to(device)\n",
    "            mask            = torch.FloatTensor(mask).to(device)\n",
    "            \n",
    "            \n",
    "            # Train the Behaviour cloning policy to be able to take more than 1 sample for MMD\n",
    "            recon, mean, std = self.vae(state, action)\n",
    "            recon_loss = F.mse_loss(recon, action)\n",
    "            KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean()\n",
    "            vae_loss = recon_loss + 0.5 * KL_loss\n",
    "\n",
    "            self.vae_optimizer.zero_grad()\n",
    "            vae_loss.backward()\n",
    "            self.vae_optimizer.step()\n",
    "\n",
    "            # Critic Training: In this step, we explicitly compute the actions \n",
    "            with torch.no_grad():\n",
    "                # Duplicate state 10 times (10 is a hyperparameter chosen by BCQ)\n",
    "                state_rep = torch.FloatTensor(np.repeat(next_state_np, 10, axis=0)).to(device)\n",
    "                \n",
    "                # Compute value of perturbed actions sampled from the VAE\n",
    "                target_Qs = self.critic_target(state_rep, self.actor_target(state_rep))\n",
    "\n",
    "                # Soft Clipped Double Q-learning \n",
    "                target_Q = 0.75 * target_Qs.min(0)[0] + 0.25 * target_Qs.max(0)[0]\n",
    "                target_Q = target_Q.view(batch_size, -1).max(1)[0].view(-1, 1)\n",
    "                target_Q = reward + done * discount * target_Q\n",
    "\n",
    "            current_Qs = self.critic(state, action, with_var=False)\n",
    "            if self.use_bootstrap: \n",
    "                critic_loss = (F.mse_loss(current_Qs[0], target_Q, reduction='none') * mask[:, 0:1]).mean() +\\\n",
    "                            (F.mse_loss(current_Qs[1], target_Q, reduction='none') * mask[:, 1:2]).mean() \n",
    "                            # (F.mse_loss(current_Qs[2], target_Q, reduction='none') * mask[:, 2:3]).mean() +\\\n",
    "                            # (F.mse_loss(current_Qs[3], target_Q, reduction='none') * mask[:, 3:4]).mean()\n",
    "            else:\n",
    "                critic_loss = F.mse_loss(current_Qs[0], target_Q) + F.mse_loss(current_Qs[1], target_Q) #+ F.mse_loss(current_Qs[2], target_Q) + F.mse_loss(current_Qs[3], target_Q)\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Action Training\n",
    "            # If you take less samples (but not too less, else it becomes statistically inefficient), it is closer to a uniform support set matching\n",
    "            num_samples = self.num_samples_match\n",
    "            sampled_actions, raw_sampled_actions = self.vae.decode_multiple(state, num_decode=num_samples)  # B x N x d\n",
    "            actor_actions, raw_actor_actions = self.actor.sample_multiple(state, num_samples)#  num)\n",
    "\n",
    "            # MMD done on raw actions (before tanh), to prevent gradient dying out due to saturation\n",
    "            if self.use_kl:\n",
    "                mmd_loss = self.kl_loss(raw_sampled_actions, state)\n",
    "            else:\n",
    "                if self.kernel_type == 'gaussian':\n",
    "                    mmd_loss = self.mmd_loss_gaussian(raw_sampled_actions, raw_actor_actions, sigma=self.mmd_sigma)\n",
    "                else:\n",
    "                    mmd_loss = self.mmd_loss_laplacian(raw_sampled_actions, raw_actor_actions, sigma=self.mmd_sigma)\n",
    "\n",
    "            action_divergence = ((sampled_actions - actor_actions)**2).sum(-1)\n",
    "            raw_action_divergence = ((raw_sampled_actions - raw_actor_actions)**2).sum(-1)\n",
    "\n",
    "            # Update through TD3 style\n",
    "            critic_qs, std_q = self.critic.q_all(state, actor_actions[:, 0, :], with_var=True)\n",
    "            critic_qs = self.critic.q_all(state.unsqueeze(0).repeat(num_samples, 1, 1).view(num_samples*state.size(0), state.size(1)), actor_actions.permute(1, 0, 2).contiguous().view(num_samples*actor_actions.size(0), actor_actions.size(2)))\n",
    "            critic_qs = critic_qs.view(self.num_qs, num_samples, actor_actions.size(0), 1)\n",
    "            critic_qs = critic_qs.mean(1)\n",
    "            std_q = torch.std(critic_qs, dim=0, keepdim=False, unbiased=False)\n",
    "\n",
    "            if not self.use_ensemble:\n",
    "                std_q = torch.zeros_like(std_q).to(device)\n",
    "                \n",
    "            if self.version == '0':\n",
    "                critic_qs = critic_qs.min(0)[0]\n",
    "            elif self.version == '1':\n",
    "                critic_qs = critic_qs.max(0)[0]\n",
    "            elif self.version == '2':\n",
    "                critic_qs = critic_qs.mean(0)\n",
    "\n",
    "            # We do support matching with a warmstart which happens to be reasonable around epoch 20 during training\n",
    "            if self.epoch >= 20: \n",
    "                if self.mode == 'auto':\n",
    "                    actor_loss = (-critic_qs +\\\n",
    "                        self._lambda * (np.sqrt((1 - self.delta_conf)/self.delta_conf)) * std_q +\\\n",
    "                        self.log_lagrange2.exp() * mmd_loss).mean()\n",
    "                else:\n",
    "                    actor_loss = (-critic_qs +\\\n",
    "                        self._lambda * (np.sqrt((1 - self.delta_conf)/self.delta_conf)) * std_q +\\\n",
    "                        100.0*mmd_loss).mean()      # This coefficient is hardcoded, and is different for different tasks. I would suggest using auto, as that is the one used in the paper and works better.\n",
    "            else:\n",
    "                if self.mode == 'auto':\n",
    "                    actor_loss = (self.log_lagrange2.exp() * mmd_loss).mean()\n",
    "                else:\n",
    "                    actor_loss = 100.0*mmd_loss.mean()\n",
    "\n",
    "            std_loss = self._lambda*(np.sqrt((1 - self.delta_conf)/self.delta_conf)) * std_q.detach() \n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            if self.mode =='auto':\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "            else:\n",
    "                actor_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm(self.actor.parameters(), 10.0)\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Threshold for the lagrange multiplier\n",
    "            thresh = 0.05\n",
    "            if self.use_kl:\n",
    "                thresh = -2.0\n",
    "\n",
    "            if self.mode == 'auto':\n",
    "                lagrange_loss = (-critic_qs +\\\n",
    "                        self._lambda * (np.sqrt((1 - self.delta_conf)/self.delta_conf)) * (std_q) +\\\n",
    "                        self.log_lagrange2.exp() * (mmd_loss - thresh)).mean()\n",
    "\n",
    "                self.lagrange2_opt.zero_grad()\n",
    "                (-lagrange_loss).backward()\n",
    "                # self.lagrange1_opt.step()\n",
    "                self.lagrange2_opt.step() \n",
    "                self.log_lagrange2.data.clamp_(min=-5.0, max=self.lagrange_thresh)   \n",
    "            \n",
    "            # Update Target Networks \n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        # Do all logging here\n",
    "        logger.record_dict(create_stats_ordered_dict(\n",
    "            'Q_target',\n",
    "            target_Q.cpu().data.numpy(),\n",
    "        ))\n",
    "        if self.mode == 'auto':\n",
    "            # logger.record_tabular('Lagrange1', self.log_lagrange1.exp().cpu().data.numpy())\n",
    "            logger.record_tabular('Lagrange2', self.log_lagrange2.exp().cpu().data.numpy())\n",
    "\n",
    "        logger.record_tabular('Actor Loss', actor_loss.cpu().data.numpy())\n",
    "        logger.record_tabular('Critic Loss', critic_loss.cpu().data.numpy())\n",
    "        logger.record_tabular('Std Loss', std_loss.cpu().data.numpy().mean())\n",
    "        logger.record_dict(create_stats_ordered_dict(\n",
    "            'MMD Loss',\n",
    "            mmd_loss.cpu().data.numpy()\n",
    "        ))\n",
    "        logger.record_dict(create_stats_ordered_dict(\n",
    "            'Sampled Actions',\n",
    "            sampled_actions.cpu().data.numpy()\n",
    "        ))\n",
    "        logger.record_dict(create_stats_ordered_dict(\n",
    "            'Actor Actions',\n",
    "            actor_actions.cpu().data.numpy()\n",
    "        ))\n",
    "        logger.record_dict(create_stats_ordered_dict(\n",
    "            'Current_Q',\n",
    "            current_Qs.cpu().data.numpy()\n",
    "        ))\n",
    "        logger.record_dict(create_stats_ordered_dict(\n",
    "            'Action_Divergence',\n",
    "            action_divergence.cpu().data.numpy()\n",
    "        ))\n",
    "        logger.record_dict(create_stats_ordered_dict(\n",
    "            'Raw Action_Divergence',\n",
    "            raw_action_divergence.cpu().data.numpy()\n",
    "        ))\n",
    "        self.epoch = self.epoch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs policy for X episodes and returns average reward\n",
    "def evaluate_policy(policy, eval_episodes=10, discounted=False, gamma=0.99):\n",
    "    avg_reward = 0.\n",
    "    all_rewards = []\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        cntr = 0\n",
    "        gamma_t = 1 # discounted\n",
    "        while ((not done)):\n",
    "            action = policy.select_action(np.array(obs))\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            if discounted:\n",
    "                avg_reward += (gamma_t * reward) # discounted\n",
    "                gamma_t = gamma * gamma_t # discounted\n",
    "            else:\n",
    "                avg_reward += reward\n",
    "            cntr += 1\n",
    "        all_rewards.append(avg_reward)\n",
    "    avg_reward /= eval_episodes\n",
    "    for j in range(eval_episodes-1, 1, -1):\n",
    "        all_rewards[j] = all_rewards[j] - all_rewards[j-1]\n",
    "\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    std_rewards = np.std(all_rewards)\n",
    "    median_reward = np.median(all_rewards)\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Evaluation over %d episodes: %f\" % (eval_episodes, avg_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward, std_rewards, median_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bear_main(env_name='Pendulum-v0', seed=0, buffer_type=\"Robust\", eval_freq=5e3, \n",
    "              max_timesteps=1e6, buffer_name=None, version='0', lamda=0.5, threshold=0.05, use_bootstrap=False,\n",
    "              algo_name=\"OursBCQ\", mode='hardcoded', num_samples_match=10, mmd_sigma=10.0, kernel_type='laplacian',\n",
    "              lagrange_thresh=10.0, distance_type=\"MMD\", log_dir='./data_hopper/', use_ensemble_variance='True',\n",
    "              use_behaviour_policy='False', cloning=\"False\", num_random=10, margin_threshold=10):\n",
    "    \n",
    "    # Generate file name\n",
    "    file_name = algo_name + \"_%s_%s_%s_%s_%s_%s_%s_%s_%s_%s_%s_%s_%s_%s_0.1\" % (env_name, str(seed), str(version), str(lamda), str(threshold), str(use_bootstrap), str(mode),\\\n",
    "         str(kernel_type), str(num_samples_match), str(mmd_sigma), str(lagrange_thresh), str(distance_type), str(use_behaviour_policy), str(num_random))\n",
    "    \n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Settings: \" + file_name)\n",
    "    print (\"---------------------------------------\")\n",
    "    \n",
    "    if not os.path.exists(\"./results\"):\n",
    "        os.makedirs(\"./results\")\n",
    "    \n",
    "    # Setup enve\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0] \n",
    "    max_action = float(env.action_space.high[0])\n",
    "    print (state_dim, action_dim)\n",
    "    print ('Max action: ', max_action)\n",
    "    \n",
    "    # Set seeds\n",
    "    seed = np.random.randint(10, 1000)\n",
    "    env.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Setup logger\n",
    "    variant = dict(\n",
    "        algorithm=algo_name,\n",
    "        version=version,\n",
    "        env_name=env_name,\n",
    "        seed=seed,\n",
    "        lamda=lamda,\n",
    "        threshold=threshold,\n",
    "        use_bootstrap=str(use_bootstrap),\n",
    "        bootstrap_dim=4,\n",
    "        delta_conf=0.1,\n",
    "        mode=mode,\n",
    "        kernel_type=kernel_type,\n",
    "        num_samples_match=num_samples_match,\n",
    "        mmd_sigma=mmd_sigma,\n",
    "        lagrange_thresh=lagrange_thresh,\n",
    "        distance_type=distance_type,\n",
    "        use_ensemble_variance=use_ensemble_variance,\n",
    "        use_data_policy=use_behaviour_policy,\n",
    "        num_random=num_random,\n",
    "        margin_threshold=margin_threshold,\n",
    "    )\n",
    "    setup_logger(file_name, variant=variant, log_dir=log_dir + file_name)\n",
    "    \n",
    "    # Setup policy (only BEAR available)\n",
    "    policy = BEAR(2, state_dim, action_dim, max_action, delta_conf=0.1, use_bootstrap=False,\n",
    "                  version=version,\n",
    "                  lambda_=float(lamda),\n",
    "                  threshold=float(threshold),\n",
    "                  mode=mode,\n",
    "                  num_samples_match=num_samples_match,\n",
    "                  mmd_sigma=mmd_sigma,\n",
    "                  lagrange_thresh=lagrange_thresh,\n",
    "                  use_kl=(True if distance_type == \"KL\" else False),\n",
    "                  use_ensemble=(False if use_ensemble_variance == \"False\" else True),\n",
    "                  kernel_type=kernel_type)\n",
    "    \n",
    "    replay_buffer = ReplayBuffer()\n",
    "    replay_buffer.load(buffer_name, bootstrap_dim=4)\n",
    "    \n",
    "    # Evaluat Algorithem\n",
    "    evaluations = []\n",
    "    episode_num = 0\n",
    "    done = True \n",
    "    training_iters = 0\n",
    "    while training_iters < max_timesteps: \n",
    "        pol_vals = policy.train(replay_buffer, iterations=int(eval_freq))\n",
    "        ret_eval, var_ret, median_ret = evaluate_policy(policy)\n",
    "        evaluations.append(ret_eval)\n",
    "        np.save(\"./results/\" + file_name, evaluations)\n",
    "        training_iters += eval_freq\n",
    "        print (\"Training iterations: \" + str(training_iters))\n",
    "        logger.record_tabular('Training Epochs', int(training_iters // int(eval_freq)))\n",
    "        logger.record_tabular('AverageReturn', ret_eval)\n",
    "        logger.record_tabular('VarianceReturn', var_ret)\n",
    "        logger.record_tabular('MedianReturn', median_ret)\n",
    "        logger.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: OursBCQ_Pendulum-v0_0_0_0.5_0.05_False_auto_gaussian_5_20.0_10.0_MMD_False_10_0.1\n",
      "---------------------------------------\n",
      "3 1\n",
      "Max action:  2.0\n",
      "2020-05-07 14:49:36.356053 CEST | [OursBCQ_Pendulum-v0_0_0_0.5_0.05_False_auto_gaussian_5_20.0_10.0_MMD_False_10_0.1] [OursBCQ_Pendulum-v0_0_0_0.5_0.05_False_auto_gaussian_5_20.0_10.0_MMD_False_10_0.1] Variant:\n",
      "2020-05-07 14:49:36.356567 CEST | [OursBCQ_Pendulum-v0_0_0_0.5_0.05_False_auto_gaussian_5_20.0_10.0_MMD_False_10_0.1] [OursBCQ_Pendulum-v0_0_0_0.5_0.05_False_auto_gaussian_5_20.0_10.0_MMD_False_10_0.1] {\n",
      "  \"algorithm\": \"OursBCQ\",\n",
      "  \"version\": 0,\n",
      "  \"env_name\": \"Pendulum-v0\",\n",
      "  \"seed\": 670,\n",
      "  \"lamda\": 0.5,\n",
      "  \"threshold\": 0.05,\n",
      "  \"use_bootstrap\": \"False\",\n",
      "  \"bootstrap_dim\": 4,\n",
      "  \"delta_conf\": 0.1,\n",
      "  \"mode\": \"auto\",\n",
      "  \"kernel_type\": \"gaussian\",\n",
      "  \"num_samples_match\": 5,\n",
      "  \"mmd_sigma\": 20.0,\n",
      "  \"lagrange_thresh\": 10.0,\n",
      "  \"distance_type\": \"MMD\",\n",
      "  \"use_ensemble_variance\": \"True\",\n",
      "  \"use_data_policy\": \"False\",\n",
      "  \"num_random\": 10,\n",
      "  \"margin_threshold\": 10\n",
      "}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [100 x 14], m2: [4 x 750] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b637b7bd03b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbear_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pendulum-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/buffers/Pendulum-v0_expert_1E06.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmd_sigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gaussian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagrange_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-ec2cb7f7e58c>\u001b[0m in \u001b[0;36mbear_main\u001b[0;34m(env_name, seed, buffer_type, eval_freq, max_timesteps, buffer_name, version, lamda, threshold, use_bootstrap, algo_name, mode, num_samples_match, mmd_sigma, kernel_type, lagrange_thresh, distance_type, log_dir, use_ensemble_variance, use_behaviour_policy, cloning, num_random, margin_threshold)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtraining_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtraining_iters\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mpol_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mret_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedian_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mevaluations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-443898189383>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# Train the Behaviour cloning policy to be able to take more than 1 sample for MMD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mrecon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mKL_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/venv_mujoco/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/off_policy_benchmark/BEAR/algos.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/venv_mujoco/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/venv_mujoco/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/venv_mujoco/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [100 x 14], m2: [4 x 750] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "bear_main(env_name='Pendulum-v0', buffer_name=\"./data/buffers/Pendulum-v0_expert_1E06.npy\", mmd_sigma=20.0, kernel_type=\"gaussian\", num_samples_match=5, version=0, lagrange_thresh=10.0, mode=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if you want to eval other algos\n",
    "\n",
    "#     # Setup policy\n",
    "#     if algo_name == 'BCQ':\n",
    "#         policy = algos.BCQ(state_dim, action_dim, max_action)\n",
    "#     elif algo_name == 'BC':\n",
    "#         policy = algos.BCQ(state_dim, action_dim, max_action, cloning=True)\n",
    "#     elif algo_name == 'DQfD':\n",
    "#         policy = algos.DQfD(state_dim, action_dim, max_action, lambda_=args.lamda, margin_threshold=float(args.margin_threshold))\n",
    "#     elif algo_name == 'KLControl':\n",
    "#         policy = algos.KLControl(2, state_dim, action_dim, max_action)\n",
    "#     elif algo_name == 'BEAR':\n",
    "#         policy = algos.BEAR(2, state_dim, action_dim, max_action, delta_conf=0.1, use_bootstrap=False,\n",
    "#             version=args.version,\n",
    "#             lambda_=float(args.lamda),\n",
    "#             threshold=float(args.threshold),\n",
    "#             mode=args.mode,\n",
    "#             num_samples_match=args.num_samples_match,\n",
    "#             mmd_sigma=args.mmd_sigma,\n",
    "#             lagrange_thresh=args.lagrange_thresh,\n",
    "#             use_kl=(True if args.distance_type == \"KL\" else False),\n",
    "#             use_ensemble=(False if args.use_ensemble_variance == \"False\" else True),\n",
    "#             kernel_type=args.kernel_type)\n",
    "#     elif algo_name == 'BEAR_IS':\n",
    "#         policy = algos.BEAR_IS(2, state_dim, action_dim, max_action, delta_conf=0.1, use_bootstrap=False,\n",
    "#             version=args.version,\n",
    "#             lambda_=float(args.lamda),\n",
    "#             threshold=float(args.threshold),\n",
    "#             mode=args.mode,\n",
    "#             num_samples_match=args.num_samples_match,\n",
    "#             mmd_sigma=args.mmd_sigma,\n",
    "#             lagrange_thresh=args.lagrange_thresh,\n",
    "#             use_kl=(True if args.distance_type == \"KL\" else False),\n",
    "#             use_ensemble=(False if args.use_ensemble_variance == \"False\" else True),\n",
    "#             kernel_type=args.kernel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegularActor(\n",
       "  (l1): Linear(in_features=10, out_features=400, bias=True)\n",
       "  (l2): Linear(in_features=400, out_features=300, bias=True)\n",
       "  (mean): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (log_std): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algos.RegularActor(10, 2, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleCritic(\n",
       "  (l1): Linear(in_features=12, out_features=400, bias=True)\n",
       "  (l2): Linear(in_features=400, out_features=300, bias=True)\n",
       "  (l3): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (l4): Linear(in_features=12, out_features=400, bias=True)\n",
       "  (l5): Linear(in_features=400, out_features=300, bias=True)\n",
       "  (l6): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algos.EnsembleCritic(4, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (e1): Linear(in_features=12, out_features=750, bias=True)\n",
       "  (e2): Linear(in_features=750, out_features=750, bias=True)\n",
       "  (mean): Linear(in_features=750, out_features=5, bias=True)\n",
       "  (log_std): Linear(in_features=750, out_features=5, bias=True)\n",
       "  (d1): Linear(in_features=15, out_features=750, bias=True)\n",
       "  (d2): Linear(in_features=750, out_features=750, bias=True)\n",
       "  (d3): Linear(in_features=750, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algos.VAE(10, 2, 5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
